{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79be9812-4d03-4df8-aa7b-0255626b4105",
   "metadata": {},
   "source": [
    "# Gesture Recognition using Google Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559a671-f39f-4ad3-ac4a-20227c0cc3ac",
   "metadata": {},
   "source": [
    "## API Call to server\n",
    "\n",
    "Everytime our model recognizes a gesture, we need it to make an api call to our custom backend so that we can recieve the data which will then trigger a python script to access AI assistant functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8a4b83c-cc00-492e-9928-75e593a1d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# The URL to which the request will be sent\n",
    "URL = \"http://localhost:8000/api/gesture\"\n",
    "\n",
    "def trigger_ai_assistant(gesture_name):\n",
    "    # The data payload of the request, with the key 'gesture' and its value\n",
    "    data = { \"gesture\": gesture_name }\n",
    "    \n",
    "    # Making the POST request\n",
    "    response = requests.post(URL, json=data)\n",
    "    \n",
    "    # Checking the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Success:\", response.json())\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ca484-393a-48d5-a600-244fb2e4c546",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbed9d81-b3b0-4b72-ae86-c7809412934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.tasks.python import text\n",
    "from mediapipe.tasks.python import audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10160545-d139-405f-aa09-4b9b8738e4f7",
   "metadata": {},
   "source": [
    "## Initializing model path\n",
    "\n",
    "We are using Google's gesture recognition model: `gesture.recognizer.task`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82fe83fb-a107-4fb5-a77c-61e17764a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'gesture_recognizer.task'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db532fc-8f5a-43f4-b1af-00afec3eb16c",
   "metadata": {},
   "source": [
    "## Create the task\n",
    "\n",
    "The MediaPipe Gesture Recognizer task uses the `create_from_options` function to set up the task. The `create_from_options` function accepts values for configuration options to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56e46122-eba9-437b-840d-ca93446c706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386fc0c-16f0-4bca-990d-8ab480ebc0f2",
   "metadata": {},
   "source": [
    "Creating a gesture recognizer instance with the live stream mode. This function runs every time our model processes a frame from the live video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "564ff523-9beb-48ee-acdd-e0c7e0066565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_process(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    print(result.gestures)\n",
    "    # Only run if the model picks up any gestures at all\n",
    "    if len(result.gestures) > 0 and result.gestures[0][0].category_name != \"None\":\n",
    "        # We use the first gesture that the model picks because it might pick multiple gestures from multiple people.\n",
    "        model_gesture_prediction = result.gestures[0][0].category_name\n",
    "        # Trigger the AI assistant using the gesture\n",
    "        trigger_ai_assistant(model_gesture_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4cfb9f-c9b1-4c17-bf2f-0b579ede5e8e",
   "metadata": {},
   "source": [
    "Configuring options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00ee77d8-c626-41b1-bf48-94d47bd55540",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM, # Using the live stream running mode so that we can use the model with openCV\n",
    "    result_callback=handle_process # Call the callback function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0f388-f2ce-4f1b-b7b5-04d0c3b3cf02",
   "metadata": {},
   "source": [
    "## OpenCV live video stream\n",
    "\n",
    "This is the most important part of the code where we setup a live stream using OpenCV and interpret a frame every `frame_interval` seconds and recognize the gesture using mediapipe ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "001f9e32-9d6f-4ff3-ab7e-de438914145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1710096551.923319  994171 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "W0000 00:00:1710096551.939883  994171 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1710096551.950516  994171 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[Category(index=-1, score=0.6295751929283142, display_name='', category_name='Open_Palm')]]\n",
      "Success: {'message': 'Gesture processed successfully', 'received_gesture': 'Open_Palm'}\n",
      "[[Category(index=-1, score=0.9680280685424805, display_name='', category_name='None')]]\n",
      "[[Category(index=-1, score=0.6391464471817017, display_name='', category_name='Thumb_Up')]]\n",
      "Success: {'message': 'Gesture processed successfully', 'received_gesture': 'Thumb_Up'}\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "    # Use OpenCV’s VideoCapture to start capturing from the webcam.\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    start_time = time.time() # start time is when we start the live stream\n",
    "    frame_interval = 2.5  # Time interval before we process the next frame for gesture recognition\n",
    "    last_frame_time = 0 # Variable used to capture a frame every 5 seconds\n",
    "\n",
    "    # Create a loop to read the latest frame from the camera using VideoCapture#read()\n",
    "    while cap.isOpened():\n",
    "        # Read a frame\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        # Ignore the empty camera frame\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the elapsed time since the start.\n",
    "        elapsed_time = time.time() - start_time  \n",
    "        \n",
    "        current_time = time.time()\n",
    "        # Display the frame.\n",
    "        cv2.imshow('MediaPipe Hands', frame)\n",
    "\n",
    "        # Only process the frame if frame_interval has passed\n",
    "        if (current_time - last_frame_time) > frame_interval:   \n",
    "            # Convert elapsed time to milliseconds.\n",
    "            frame_timestamp_ms = int(elapsed_time * 1000)  \n",
    "    \n",
    "            # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "            # Use the model to recognize the gesture in the frame and call the callback function\n",
    "            recognizer.recognize_async(mp_image, frame_timestamp_ms)\n",
    "\n",
    "            # Update the last frame time so that this snippet runs again after 'frame_interval' seconds\n",
    "            last_frame_time = current_time\n",
    "        \n",
    "        # Break the loop when 'q' is pressed.\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the webcam and close the window.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97289ae2-96d0-4481-bc85-2cbd21f9140b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signify_environment",
   "language": "python",
   "name": "signify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
