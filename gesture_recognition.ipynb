{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbed9d81-b3b0-4b72-ae86-c7809412934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.tasks.python import text\n",
    "from mediapipe.tasks.python import audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82fe83fb-a107-4fb5-a77c-61e17764a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './gesture_recognizer.task'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dee1dc2-e1d1-433f-ac21-173a2b857657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1709664643.847211       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "W0000 00:00:1709664643.856496       1 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1709664643.862338       1 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n"
     ]
    }
   ],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a gesture recognizer instance with the live stream mode:\n",
    "def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    # print('gesture recognition result: {}'.format(result))\n",
    "    pass\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result)\n",
    "\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "    # Start capturing from the webcam.\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    start_time = time.time() \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        # Ignore the empty camera frame\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        current_time = time.time() - start_time  # Calculate the elapsed time since the start.\n",
    "        frame_timestamp_ms = int(current_time * 1000)  # Convert elapsed time to milliseconds.\n",
    "\n",
    "        # Convert the frame received from OpenCV to a MediaPipeâ€™s Image object.\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "        recognizer.recognize_async(mp_image, frame_timestamp_ms)\n",
    "    \n",
    "        # Display the frame.\n",
    "        cv2.imshow('MediaPipe Hands', frame)\n",
    "        \n",
    "        # Break the loop when 'q' is pressed.\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the webcam and close the window.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
